# ä¿å®ˆè‡ªç”±ç©ºé—´æ¸…ç†ç­–ç•¥çš„å¥–åŠ±å‡½æ•°è°ƒæ•´å»ºè®®

## ğŸ” é—®é¢˜è¯Šæ–­

### å½“å‰å¥–åŠ±å‡½æ•°é—®é¢˜
```python
# å½“å‰: r = 3.0*(info_gain - overlap) 
# é—®é¢˜: ä¿å®ˆç­–ç•¥ä¸‹ï¼ŒåæœŸinfo_gainâ†’0, overlapâ†’100%ï¼Œå¥–åŠ±æ’ä¸º-3.0
```

ä»æµ‹è¯•æ•°æ®çœ‹åˆ°ï¼š
- **ç¬¬6æ­¥å**ï¼šinfo_gain=0, overlap=100% â†’ å¥–åŠ±=-3.0
- **è¦†ç›–ç‡åœæ»**ï¼š6.0%å·¦å³æ— æ³•ç»§ç»­æ¢ç´¢  
- **å­¦ä¹ ä¿¡å·ä¸¢å¤±**ï¼šæ‰€æœ‰åŠ¨ä½œå¾—åˆ°ç›¸åŒè´Ÿå¥–åŠ±

## ğŸ’¡ æ¨èæ–¹æ¡ˆï¼šæ··åˆè¦†ç›–ç‡å¯¼å‘å¥–åŠ±

### æ ¸å¿ƒè®¾è®¡åŸåˆ™
1. **è¦†ç›–ç‡å¢é•¿ä¸ºä¸»**ï¼šç›´æ¥å¥–åŠ±åœ°å›¾è¦†ç›–ç‡æå‡
2. **è‡ªé€‚åº”é‡å æƒ©ç½š**ï¼šä½é‡å ç‡æ—¶ä¸æƒ©ç½šï¼Œé«˜é‡å ç‡æ—¶æ‰æƒ©ç½š
3. **æ¢ç´¢æ•ˆç‡å¥–åŠ±**ï¼šé¼“åŠ±å¿«é€Ÿæœ‰æ•ˆçš„æ¢ç´¢
4. **é˜¶æ®µæ€§è°ƒæ•´**ï¼šæ ¹æ®æ¢ç´¢è¿›åº¦è°ƒæ•´å¥–åŠ±ç­–ç•¥

### å…·ä½“å®ç°

```python
@dataclass
class RewardConfig:
    # æ¨èé…ç½®
    w_coverage: float = 10.0      # è¦†ç›–ç‡å¢é•¿ä¸»è¦å¥–åŠ±
    w_info: float = 1.0           # ä¿¡æ¯å¢ç›Šè¾…åŠ©å¥–åŠ±
    w_overlap_base: float = 1.0   # åŸºç¡€é‡å æƒ©ç½š
    overlap_threshold: float = 0.6 # é‡å ç‡å®¹å¿é˜ˆå€¼
    w_efficiency: float = 0.5     # æ—¶é—´æ•ˆç‡å¥–åŠ±
    
def enhanced_reward(info_gain, overlap, fail, dt, coverage_gain, current_coverage):
    """æ”¹è¿›çš„å¥–åŠ±å‡½æ•°"""
    
    # 1. ä¸»è¦å¥–åŠ±ï¼šè¦†ç›–ç‡å¢é•¿
    coverage_reward = 10.0 * coverage_gain
    
    # 2. è¾…åŠ©å¥–åŠ±ï¼šä¿¡æ¯å¢ç›Š
    info_reward = 1.0 * info_gain
    
    # 3. è‡ªé€‚åº”é‡å æƒ©ç½š
    if overlap <= 0.6:
        overlap_penalty = 0  # å®¹å¿åˆç†é‡å 
    else:
        overlap_penalty = 1.0 * (overlap - 0.6) / 0.4  # è¶…è¿‡é˜ˆå€¼æ‰æƒ©ç½š
    
    # 4. æ¢ç´¢æ•ˆç‡å¥–åŠ±
    efficiency_bonus = 0.5 / (dt + 0.001)
    
    # 5. é˜¶æ®µæ€§è°ƒæ•´
    if current_coverage < 0.1:
        # æ—©æœŸï¼šé¼“åŠ±å¿«é€Ÿæ¢ç´¢
        reward = coverage_reward * 1.5 + info_reward - overlap_penalty * 0.5
    elif current_coverage < 0.3:
        # ä¸­æœŸï¼šå¹³è¡¡æ¢ç´¢å’Œç²¾åº¦
        reward = coverage_reward + info_reward - overlap_penalty
    else:
        # åæœŸï¼šé‡è§†ç²¾ç»†æ‰«æ
        reward = coverage_reward * 0.8 + info_reward * 1.2 - overlap_penalty * 1.5
    
    # æ·»åŠ æ•ˆç‡å¥–åŠ±
    reward += efficiency_bonus
    
    return reward
```

### ä¸ç°æœ‰ä»£ç é›†æˆ

ä¿®æ”¹ `train_trigger_ppo.py` ä¸­çš„å¥–åŠ±è®¡ç®—ï¼š

```python
# åœ¨ç¯å¢ƒstepåï¼Œæ·»åŠ è¦†ç›–ç‡è·Ÿè¸ª
prev_coverage = getattr(env, '_prev_coverage', 0.0)  # è®°å½•ä¸Šä¸€æ­¥è¦†ç›–ç‡
current_coverage = env.coverage()
coverage_gain = max(0.0, current_coverage - prev_coverage)
env._prev_coverage = current_coverage

# ä½¿ç”¨æ”¹è¿›çš„å¥–åŠ±å‡½æ•°
r_np = enhanced_reward(
    info_gain, overlap, fail, dt, 
    coverage_gain, current_coverage
)
```

## ğŸ“Š é¢„æœŸæ•ˆæœ

### å¥–åŠ±ä¿¡å·å¯¹æ¯”
| åœºæ™¯ | åŸå§‹å¥–åŠ± | æ”¹è¿›å¥–åŠ± | æ•ˆæœ |
|------|----------|----------|------|
| æ¢ç´¢æ–°åŒºåŸŸ | +1.5 | +10.5 | å¼ºçƒˆæ­£å‘æ¿€åŠ± |
| ä½é‡å é‡æ‰« | -1.5 | +8.0 | å…è®¸åˆç†é‡å¤ |
| é«˜é‡å é‡æ‰« | -3.0 | +5.0 | ä»æœ‰æ­£å‘ä½†è¾ƒä½ |
| çº¯é‡å¤æ‰«æ | -3.0 | +2.0 | é¿å…è´Ÿå¥–åŠ±é™·é˜± |

### å­¦ä¹ åŠ¨æ€æ”¹å–„
1. **æ—©æœŸ**ï¼šå¿«é€Ÿæ‰©å±•è¦†ç›–èŒƒå›´ï¼Œå»ºç«‹åœ°å›¾æ¡†æ¶
2. **ä¸­æœŸ**ï¼šå¹³è¡¡æ–°åŒºåŸŸæ¢ç´¢å’Œå·²çŸ¥åŒºåŸŸç»†åŒ–
3. **åæœŸ**ï¼šé‡è§†æ‰«æè´¨é‡ï¼Œæé«˜åœ°å›¾ç²¾åº¦

## ğŸ”§ å®æ–½å»ºè®®

### ç«‹å³å®æ–½
1. **ä¿®æ”¹TrainConfig**ï¼šè°ƒæ•´å¥–åŠ±æƒé‡
2. **æ›´æ–°å¥–åŠ±è®¡ç®—**ï¼šé›†æˆè¦†ç›–ç‡è·Ÿè¸ª
3. **é‡æ–°è®­ç»ƒ**ï¼šä½¿ç”¨æ–°å¥–åŠ±å‡½æ•°è®­ç»ƒæ¨¡å‹

### è¿›ä¸€æ­¥ä¼˜åŒ–
1. **åŠ¨æ€æƒé‡**ï¼šæ ¹æ®è®­ç»ƒè¿›åº¦è‡ªåŠ¨è°ƒæ•´æƒé‡
2. **å¤šç›®æ ‡å¥–åŠ±**ï¼šç»“åˆè¦†ç›–ç‡ã€ç²¾åº¦ã€æ•ˆç‡å¤šä¸ªæŒ‡æ ‡
3. **è¯¾ç¨‹å­¦ä¹ **ï¼šä»ç®€å•ç¯å¢ƒé€æ­¥è¿‡æ¸¡åˆ°å¤æ‚ç¯å¢ƒ

è¿™å¥—æ”¹è¿›æ–¹æ¡ˆä¸“é—¨é’ˆå¯¹ä¿å®ˆè‡ªç”±ç©ºé—´æ¸…ç†ç­–ç•¥è®¾è®¡ï¼Œå¯ä»¥æœ‰æ•ˆè§£å†³å½“å‰çš„å¥–åŠ±é¥±å’Œå’Œå­¦ä¹ åœæ»é—®é¢˜ã€‚
